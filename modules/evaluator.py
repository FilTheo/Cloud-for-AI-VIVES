from logs.logging_config import setup_logger
from modules.utils.evaluation import Evaluator
from modules.utils.utils import pivoted_df
import pandas as pd

# Set up logger
logger = setup_logger(__name__, 'evaluator.log')

class ForecastEvaluator(object):
    """
    Evaluates predictions with the newly arrived data of last week

    The Evaluator takes the forecasts generated by a model last week.
    When new data arrives, the evaluator compares the predictions with the actual data.
    """

    def __init__(
        self,
        freq: str,
        original_df: pd.DataFrame,
    ) -> None:
        """
        Initialize the ForecastEvaluator

        Args:
        ------
        freq : str
            The frequency of the time series data.
        original_df : pd.DataFrame
            A DataFrame containing the original values.

        Returns:
        --------
        None
        """
        logger.info("Initializing ForecastEvaluator")

        # Initialize
        self.freq = freq
        # Convert to pivoted format for easier processing
        self.original_df = pivoted_df(original_df)
        
        logger.info(f"ForecastEvaluator initialized with frequency: {freq}")
        logger.info(f"Original DataFrame shape: {self.original_df.shape}")

    def fit(
        self,
        forecast_df: pd.DataFrame = None,
        latest_true_df: pd.DataFrame = None,
        evaluation_cv: bool = False,
        complete_evaluation_df: pd.DataFrame = None,
    ) -> pd.DataFrame:
        """
        Fit the evaluator:
        In case we want to do cross-validation we provide the complete evaluation dataframe.

        Args:
        ------
        forecast_df : pd.DataFrame
            A DataFrame containing the forecasted values.
        latest_true_df : pd.DataFrame
            A DataFrame containing the actual values.
        evaluation_cv : bool
            A boolean indicating whether to do cross-validation or not.
        complete_evaluation_df : pd.DataFrame
            A DataFrame containing the complete evaluation df in case we do cross-validation

        Returns
        -------
        None
        """
        logger.info("Fitting the evaluator")

        # If we evaluate on a cv
        if evaluation_cv:
            logger.info("Performing cross-validation evaluation")
            # Assert we have the complete evaluation dataframe
            assert (
                complete_evaluation_df is not None
            ), "Please provide the complete evaluation dataframe"
            self.complete_evaluation_df = complete_evaluation_df
            logger.info(f"Complete evaluation DataFrame shape: {self.complete_evaluation_df.shape}")

        else:
            logger.info("Performing standard evaluation")
            # Assert that both forecast_df and latest_true_df are not None
            assert (
                forecast_df is not None and latest_true_df is not None
            ), "Please provide both forecast_df and latest_true_df"

            # Prepare the latest true data
            try:
                logger.info("Renaming columns")
                latest_true_df = latest_true_df.rename(columns={"y": "True"})
                latest_true_df = latest_true_df[
                    ["unique_id", "date", "True"]
                ]  # Keep only the relevant columns

                # Merge forecast and true data
                logger.info("Merging forecast and true data")
                self.complete_evaluation_df = pd.merge(
                    forecast_df, latest_true_df, on=["unique_id", "date"], how="left"
                )
                # A hack to bypass the update in DeepRetail
                self.complete_evaluation_df["cv"] = self.complete_evaluation_df[
                    "cv"
                ].fillna(1)

                # fillnas on True for no sale periods
                self.complete_evaluation_df["True"] = self.complete_evaluation_df["True"].fillna(0)

                logger.info(f"Merged evaluation DataFrame shape: {self.complete_evaluation_df.shape}")

            except Exception as e:
                logger.error(f"Error merging forecast and true data: {e}")
                raise e


        # Define the evaluator
        logger.info("Initializing DeepRetail Evaluator")
        self.evaluator = Evaluator(
            self.original_df,
            self.complete_evaluation_df,
            freq=self.freq,
            format="pivoted",
        )
        # A small hack to bypass an update to DeepRetail
        # self.evaluator.total_cv = 0

        logger.info("Evaluator fitting completed")

    def predict(self, metrics: list = None) -> pd.DataFrame:
        """
        Evaluate the forecast

        Args:
        ------
        metrics : List[str]
            A list of metrics to evaluate the forecast.

        Returns
        -------
        evaluation_df : pd.DataFrame
            A DataFrame containing the evaluation metrics.
        """
        logger.info("Starting forecast evaluation")
        
        if metrics:
            logger.info(f"Evaluating with metrics: {metrics}")
        else:
            logger.info("No specific metrics provided, using default metrics")

        evaluation_df = self.evaluator.evaluate(metrics=metrics)

        # Add a column with the max date
        evaluation_df["date"] = self.complete_evaluation_df["date"].max()
        
        logger.info(f"Evaluation completed. Evaluation DataFrame shape: {evaluation_df.shape}")
        return evaluation_df
